/**
 *   @ingroup hal
 *   @file
 *
 *   ARTI -- RTAI-compatible Adeos-based Real-Time Interface. Based on
 *   the original RTAI layer for x86.
 *
 *   Original RTAI/x86 layer implementation: \n
 *   Copyright &copy; 2000 Paolo Mantegazza, \n
 *   Copyright &copy; 2000 Steve Papacharalambous, \n
 *   Copyright &copy; 2000 Stuart Hughes, \n
 *   and others.
 *
 *   RTAI/x86 rewrite over Adeos: \n
 *   Copyright &copy 2002 Philippe Gerum.
 *
 *   Porting to x86_64 architecture:
 *   Copyright &copy; 2005 Paolo Mantegazza, \n
 *   Copyright &copy; 2005 Daniele Gasperini \n
 *
 *   This program is free software; you can redistribute it and/or modify
 *   it under the terms of the GNU General Public License as published by
 *   the Free Software Foundation, Inc., 675 Mass Ave, Cambridge MA 02139,
 *   USA; either version 2 of the License, or (at your option) any later
 *   version.
 *
 *   This program is distributed in the hope that it will be useful,
 *   but WITHOUT ANY WARRANTY; without even the implied warranty of
 *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *   GNU General Public License for more details.
 *
 *   You should have received a copy of the GNU General Public License
 *   along with this program; if not, write to the Free Software
 *   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
 */

/**
 * @defgroup hal RTAI services functions.
 *
 * This module defines some functions that can be used by RTAI tasks, for
 * managing interrupts and communication services with Linux processes.
 *
 *@{*/


#if 0	/* defined & set in rtai_config.h */
#define CONFIG_RTAI_DONT_DISPATCH_CORE_IRQS  0
#endif

#include <linux/version.h>
#include <linux/slab.h>
#include <linux/errno.h>
#include <linux/module.h>
#include <linux/interrupt.h>
#include <linux/irq.h>
#include <linux/console.h>
#include <asm/system.h>
#include <asm/hw_irq.h>
#include <asm/irq.h>
#include <asm/desc.h>
#include <asm/io.h>
#include <asm/mmu_context.h>
#include <asm/uaccess.h>
#include <asm/unistd.h>
#ifdef CONFIG_X86_LOCAL_APIC
#include <asm/fixmap.h>
#include <asm/bitops.h>
#include <asm/mpspec.h>
#ifdef CONFIG_X86_IO_APIC
#include <asm/io_apic.h>
#endif /* CONFIG_X86_IO_APIC */
#include <asm/apic.h>
#endif /* CONFIG_X86_LOCAL_APIC */
#define __RTAI_HAL__
#include <asm/rtai_hal.h>
#include <asm/rtai_lxrt.h>
#ifdef CONFIG_PROC_FS
#include <linux/stat.h>
#include <linux/proc_fs.h>
#include <rtai_proc_fs.h>
#endif /* CONFIG_PROC_FS */
#include <stdarg.h>

MODULE_LICENSE("GPL");

static unsigned long rtai_cpufreq_arg = RTAI_CALIBRATED_CPU_FREQ;
RTAI_MODULE_PARM(rtai_cpufreq_arg, ulong);

#define RTAI_NR_IRQS  IPIPE_NR_XIRQS

#ifdef CONFIG_X86_LOCAL_APIC

static unsigned long rtai_apicfreq_arg = RTAI_CALIBRATED_APIC_FREQ;
RTAI_MODULE_PARM(rtai_apicfreq_arg, ulong);

static int rtai_request_tickdev(void);

static void rtai_release_tickdev(void);

static inline void rtai_setup_periodic_apic (unsigned count, unsigned vector)
{
	apic_read(APIC_LVTT);
	apic_write(APIC_LVTT, APIC_LVT_TIMER_PERIODIC | vector);
	apic_read(APIC_TMICT);
	apic_write(APIC_TMICT, count);
}

static inline void rtai_setup_oneshot_apic (unsigned count, unsigned vector)
{
	apic_read(APIC_LVTT);
	apic_write(APIC_LVTT, vector);
	apic_read(APIC_TMICT);
	apic_write(APIC_TMICT, count);
}

#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,11)
#define __ack_APIC_irq  ack_APIC_irq
#endif

#else /* !CONFIG_X86_LOCAL_APIC */

#define rtai_setup_periodic_apic(count, vector)

#define rtai_setup_oneshot_apic(count, vector)

#define __ack_APIC_irq()

#endif /* CONFIG_X86_LOCAL_APIC */

struct { volatile int locked, rqsted; } rt_scheduling[RTAI_NR_CPUS];

#ifdef CONFIG_RTAI_SCHED_ISR_LOCK
static void (*rtai_isr_hook)(int cpuid);
#endif /* CONFIG_RTAI_SCHED_ISR_LOCK */

extern struct gate_struct idt_table[];

struct hal_domain_struct rtai_domain;

struct rtai_realtime_irq_s rtai_realtime_irq[RTAI_NR_IRQS];

static struct {
	unsigned long flags;
	int count;
} rtai_linux_irq[RTAI_NR_IRQS];

static struct {
	void (*k_handler)(void);
	long long (*u_handler)(unsigned long);
	unsigned long label;
} rtai_sysreq_table[RTAI_NR_SRQS];

static unsigned rtai_sysreq_virq;

static unsigned long rtai_sysreq_map = 3; /* srqs #[0-1] are reserved */

static unsigned long rtai_sysreq_pending;

static unsigned long rtai_sysreq_running;

static spinlock_t rtai_lsrq_lock = SPIN_LOCK_UNLOCKED;

static volatile int rtai_sync_level;

static atomic_t rtai_sync_count = ATOMIC_INIT(1);

static int rtai_last_8254_counter2;

static RTIME rtai_ts_8254;

//#define RTAI_SYSCALLS_USE_HIRQ_DISPATCHER

#ifndef RTAI_SYSCALLS_USE_HIRQ_DISPATCHER
static struct gate_struct rtai_sysvec;
#endif

static RT_TRAP_HANDLER rtai_trap_handler;

struct rt_times rt_times;

struct rt_times rt_smp_times[RTAI_NR_CPUS];

struct rtai_switch_data rtai_linux_context[RTAI_NR_CPUS];

#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
volatile unsigned long *ipipe_root_status[RTAI_NR_CPUS];
#endif

struct calibration_data rtai_tunables;

volatile unsigned long rtai_cpu_realtime;

volatile unsigned long rtai_cpu_lock;

unsigned long rtai_critical_enter (void (*synch)(void))
{
	unsigned long flags;

	flags = hal_critical_enter(synch);
	if (atomic_dec_and_test(&rtai_sync_count)) {
		rtai_sync_level = 0;
	} else if (synch != NULL) {
		printk(KERN_INFO "RTAI[hal]: warning: nested sync will fail.\n");
	}
	return flags;
}

void rtai_critical_exit (unsigned long flags)
{
	atomic_inc(&rtai_sync_count);
	hal_critical_exit(flags);
}

unsigned long IsolCpusMask = 0;
RTAI_MODULE_PARM(IsolCpusMask, ulong);

int rt_request_irq (unsigned irq, int (*handler)(unsigned irq, void *cookie), void *cookie, int retmode)
{
	unsigned long flags;

	if (handler == NULL || irq >= RTAI_NR_IRQS) {
		return -EINVAL;
	}
	if (rtai_realtime_irq[irq].handler != NULL) {
		return -EBUSY;
	}
	flags = rtai_critical_enter(NULL);
	rtai_realtime_irq[irq].handler = (void *)handler;
	rtai_realtime_irq[irq].cookie  = cookie;
	rtai_realtime_irq[irq].retmode = retmode ? 1 : 0;
        rtai_realtime_irq[irq].irq_ack = hal_root_domain->irqs[irq].acknowledge;
	rtai_critical_exit(flags);
	if (IsolCpusMask && irq < IPIPE_NR_XIRQS) {
		rtai_realtime_irq[irq].cpumask = rt_assign_irq_to_cpu(irq, IsolCpusMask);
	}
	return 0;
}

int rt_release_irq (unsigned irq)
{
	unsigned long flags;
	if (irq >= RTAI_NR_IRQS || !rtai_realtime_irq[irq].handler) {
		return -EINVAL;
	}
	flags = rtai_critical_enter(NULL);
	rtai_realtime_irq[irq].handler = NULL;
	rtai_realtime_irq[irq].irq_ack = hal_root_domain->irqs[irq].acknowledge;
	rtai_critical_exit(flags);
	if (IsolCpusMask && irq < IPIPE_NR_XIRQS) {
		rt_assign_irq_to_cpu(irq, rtai_realtime_irq[irq].cpumask);
	}
	return 0;
}

int rt_set_irq_ack(unsigned irq, int (*irq_ack)(unsigned int))
{
        if (irq >= RTAI_NR_IRQS) {
                return -EINVAL;
        }
        rtai_realtime_irq[irq].irq_ack = irq_ack ? irq_ack : hal_root_domain->irqs[irq].acknowledge;
        return 0;
}

void rt_set_irq_cookie (unsigned irq, void *cookie)
{
	if (irq < RTAI_NR_IRQS) {
		rtai_realtime_irq[irq].cookie = cookie;
	}
}

void rt_set_irq_retmode (unsigned irq, int retmode)
{
	if (irq < RTAI_NR_IRQS) {
		rtai_realtime_irq[irq].retmode = retmode ? 1 : 0;
	}
}

//extern unsigned long io_apic_irqs;

#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,11)

#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,18)
#define rtai_irq_desc(irq) (irq_desc[irq].handler)
#else
#define rtai_irq_desc(irq) (irq_desc[irq].chip)
#endif

#define BEGIN_PIC()
#define END_PIC()
#undef hal_lock_irq
#undef hal_unlock_irq
#define hal_lock_irq(x, y, z)
#define hal_unlock_irq(x, y)

#else /* LINUX_VERSION_CODE < KERNEL_VERSION(2,6,11) */

extern struct hw_interrupt_type hal_std_irq_dtype[];
#define rtai_irq_desc(irq) (&hal_std_irq_dtype[irq])

#define BEGIN_PIC() \
do { \
        unsigned long flags, pflags, cpuid; \
	rtai_save_flags_and_cli(flags); \
	cpuid = rtai_cpuid(); \
	pflags = xchg(ROOT_STATUS_ADR(cpuid), 1 << IPIPE_STALL_FLAG); \
	rtai_save_and_lock_preempt_count()

#define END_PIC() \
	rtai_restore_preempt_count(); \
	ROOT_STATUS_VAL(cpuid) = pflags; \
	rtai_restore_flags(flags); \
} while (0)

#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2,6,11) */

/**
 * start and initialize the PIC to accept interrupt request irq.
 *
 * The above function allow you to manipulate the PIC at hand, but you must
 * know what you are doing. Such a duty does not pertain to this manual and
 * you should refer to your PIC datasheet.
 *
 * Note that Linux has the same functions, but they must be used only for its
 * interrupts. Only the above ones can be safely used in real time handlers.
 *
 * It must also be remarked that when you install a real time interrupt handler,
 * RTAI already calls either rt_mask_and_ack_irq(), for level triggered
 * interrupts, or rt_ack_irq(), for edge triggered interrupts, before passing
 * control to you interrupt handler. hus generally you should just call
 * rt_unmask_irq() at due time, for level triggered interrupts, while nothing
 * should be done for edge triggered ones. Recall that in the latter case you
 * allow also any new interrupts on the same request as soon as you enable
 * interrupts at the CPU level.
 * 
 * Often some of the above functions do equivalent things. Once more there is no
 * way of doing it right except by knowing the hardware you are manipulating.
 * Furthermore you must also remember that when you install a hard real time
 * handler the related interrupt is usually disabled, unless you are overtaking
 * one already owned by Linux which has been enabled by it.   Recall that if
 * have done it right, and interrupts do not show up, it is likely you have just
 * to rt_enable_irq() your irq.
 */
unsigned rt_startup_irq (unsigned irq)
{
        int retval;

	BEGIN_PIC();
	hal_unlock_irq(hal_root_domain, irq);
	retval = rtai_irq_desc(irq)->startup(irq);
	END_PIC();
        return retval;
}

/**
 * Shut down an IRQ source.
 *
 * No further interrupt request irq can be accepted.
 *
 * The above function allow you to manipulate the PIC at hand, but you must
 * know what you are doing. Such a duty does not pertain to this manual and
 * you should refer to your PIC datasheet.
 *
 * Note that Linux has the same functions, but they must be used only for its
 * interrupts. Only the above ones can be safely used in real time handlers.
 *
 * It must also be remarked that when you install a real time interrupt handler,
 * RTAI already calls either rt_mask_and_ack_irq(), for level triggered
 * interrupts, or rt_ack_irq(), for edge triggered interrupts, before passing
 * control to you interrupt handler. hus generally you should just call
 * rt_unmask_irq() at due time, for level triggered interrupts, while nothing
 * should be done for edge triggered ones. Recall that in the latter case you
 * allow also any new interrupts on the same request as soon as you enable
 * interrupts at the CPU level.
 * 
 * Often some of the above functions do equivalent things. Once more there is no
 * way of doing it right except by knowing the hardware you are manipulating.
 * Furthermore you must also remember that when you install a hard real time
 * handler the related interrupt is usually disabled, unless you are overtaking
 * one already owned by Linux which has been enabled by it.   Recall that if
 * have done it right, and interrupts do not show up, it is likely you have just
 * to rt_enable_irq() your irq.
 */
void rt_shutdown_irq (unsigned irq)
{
	BEGIN_PIC();
	rtai_irq_desc(irq)->shutdown(irq);
	hal_clear_irq(hal_root_domain, irq);
	END_PIC();
}

static inline void _rt_enable_irq (unsigned irq)
{
	BEGIN_PIC();
	hal_unlock_irq(hal_root_domain, irq);
	rtai_irq_desc(irq)->enable(irq);
	END_PIC();
}

/**
 * Enable an IRQ source.
 *
 * The above function allow you to manipulate the PIC at hand, but you must
 * know what you are doing. Such a duty does not pertain to this manual and
 * you should refer to your PIC datasheet.
 *
 * Note that Linux has the same functions, but they must be used only for its
 * interrupts. Only the above ones can be safely used in real time handlers.
 *
 * It must also be remarked that when you install a real time interrupt handler,
 * RTAI already calls either rt_mask_and_ack_irq(), for level triggered
 * interrupts, or rt_ack_irq(), for edge triggered interrupts, before passing
 * control to you interrupt handler. hus generally you should just call
 * rt_unmask_irq() at due time, for level triggered interrupts, while nothing
 * should be done for edge triggered ones. Recall that in the latter case you
 * allow also any new interrupts on the same request as soon as you enable
 * interrupts at the CPU level.
 * 
 * Often some of the above functions do equivalent things. Once more there is no
 * way of doing it right except by knowing the hardware you are manipulating.
 * Furthermore you must also remember that when you install a hard real time
 * handler the related interrupt is usually disabled, unless you are overtaking
 * one already owned by Linux which has been enabled by it.   Recall that if
 * have done it right, and interrupts do not show up, it is likely you have just
 * to rt_enable_irq() your irq.
 */
void rt_enable_irq (unsigned irq)
{
	_rt_enable_irq(irq);
}

/**
 * Disable an IRQ source.
 *
 * The above function allow you to manipulate the PIC at hand, but you must
 * know what you are doing. Such a duty does not pertain to this manual and
 * you should refer to your PIC datasheet.
 *
 * Note that Linux has the same functions, but they must be used only for its
 * interrupts. Only the above ones can be safely used in real time handlers.
 *
 * It must also be remarked that when you install a real time interrupt handler,
 * RTAI already calls either rt_mask_and_ack_irq(), for level triggered
 * interrupts, or rt_ack_irq(), for edge triggered interrupts, before passing
 * control to you interrupt handler. hus generally you should just call
 * rt_unmask_irq() at due time, for level triggered interrupts, while nothing
 * should be done for edge triggered ones. Recall that in the latter case you
 * allow also any new interrupts on the same request as soon as you enable
 * interrupts at the CPU level.
 * 
 * Often some of the above functions do equivalent things. Once more there is no
 * way of doing it right except by knowing the hardware you are manipulating.
 * Furthermore you must also remember that when you install a hard real time
 * handler the related interrupt is usually disabled, unless you are overtaking
 * one already owned by Linux which has been enabled by it.   Recall that if
 * have done it right, and interrupts do not show up, it is likely you have just
 * to rt_enable_irq() your irq.
 */
void rt_disable_irq (unsigned irq)
{
	BEGIN_PIC();
	rtai_irq_desc(irq)->disable(irq);
	hal_lock_irq(hal_root_domain, cpuid, irq);
	END_PIC();
}

/**
 * Mask and acknowledge and IRQ source.
 *
 * No  * other interrupts can be accepted, once also the CPU will enable
 * interrupts, which ones depends on the PIC at hand and on how it is
 * programmed.
 *
 * The above function allow you to manipulate the PIC at hand, but you must
 * know what you are doing. Such a duty does not pertain to this manual and
 * you should refer to your PIC datasheet.
 *
 * Note that Linux has the same functions, but they must be used only for its
 * interrupts. Only the above ones can be safely used in real time handlers.
 *
 * It must also be remarked that when you install a real time interrupt handler,
 * RTAI already calls either rt_mask_and_ack_irq(), for level triggered
 * interrupts, or rt_ack_irq(), for edge triggered interrupts, before passing
 * control to you interrupt handler. hus generally you should just call
 * rt_unmask_irq() at due time, for level triggered interrupts, while nothing
 * should be done for edge triggered ones. Recall that in the latter case you
 * allow also any new interrupts on the same request as soon as you enable
 * interrupts at the CPU level.
 * 
 * Often some of the above functions do equivalent things. Once more there is no
 * way of doing it right except by knowing the hardware you are manipulating.
 * Furthermore you must also remember that when you install a hard real time
 * handler the related interrupt is usually disabled, unless you are overtaking
 * one already owned by Linux which has been enabled by it.   Recall that if
 * have done it right, and interrupts do not show up, it is likely you have just
 * to rt_enable_irq() your irq.
 */
void rt_mask_and_ack_irq (unsigned irq)
{
	rtai_irq_desc(irq)->ack(irq);
}

static inline void _rt_end_irq (unsigned irq)
{
	BEGIN_PIC();
	if (
#ifdef CONFIG_X86_IO_APIC
	    !IO_APIC_IRQ(irq) ||
#endif /* CONFIG_X86_IO_APIC */
	    !(irq_desc[irq].status & (IRQ_DISABLED | IRQ_INPROGRESS))) {
		hal_unlock_irq(hal_root_domain, irq);
	}
	rtai_irq_desc(irq)->end(irq);
	END_PIC();
}

/**
 * Unmask and IRQ source.
 *
 * The related request can then interrupt the CPU again, provided it has also
 * been acknowledged.
 *
 * The above function allow you to manipulate the PIC at hand, but you must
 * know what you are doing. Such a duty does not pertain to this manual and
 * you should refer to your PIC datasheet.
 *
 * Note that Linux has the same functions, but they must be used only for its
 * interrupts. Only the above ones can be safely used in real time handlers.
 *
 * It must also be remarked that when you install a real time interrupt handler,
 * RTAI already calls either rt_mask_and_ack_irq(), for level triggered
 * interrupts, or rt_ack_irq(), for edge triggered interrupts, before passing
 * control to you interrupt handler. hus generally you should just call
 * rt_unmask_irq() at due time, for level triggered interrupts, while nothing
 * should be done for edge triggered ones. Recall that in the latter case you
 * allow also any new interrupts on the same request as soon as you enable
 * interrupts at the CPU level.
 * 
 * Often some of the above functions do equivalent things. Once more there is no
 * way of doing it right except by knowing the hardware you are manipulating.
 * Furthermore you must also remember that when you install a hard real time
 * handler the related interrupt is usually disabled, unless you are overtaking
 * one already owned by Linux which has been enabled by it.   Recall that if
 * have done it right, and interrupts do not show up, it is likely you have just
 * to rt_enable_irq() your irq.
 */
void rt_unmask_irq (unsigned irq)
{
	_rt_end_irq(irq);
}

/**
 * Acknowledge an IRQ source.
 *
 * The related request can then interrupt the CPU again, provided it has not
 * been masked.
 *
 * The above function allow you to manipulate the PIC at hand, but you must
 * know what you are doing. Such a duty does not pertain to this manual and
 * you should refer to your PIC datasheet.
 *
 * Note that Linux has the same functions, but they must be used only for its
 * interrupts. Only the above ones can be safely used in real time handlers.
 *
 * It must also be remarked that when you install a real time interrupt handler,
 * RTAI already calls either rt_mask_and_ack_irq(), for level triggered
 * interrupts, or rt_ack_irq(), for edge triggered interrupts, before passing
 * control to you interrupt handler. hus generally you should just call
 * rt_unmask_irq() at due time, for level triggered interrupts, while nothing
 * should be done for edge triggered ones. Recall that in the latter case you
 * allow also any new interrupts on the same request as soon as you enable
 * interrupts at the CPU level.
 * 
 * Often some of the above functions do equivalent things. Once more there is no
 * way of doing it right except by knowing the hardware you are manipulating.
 * Furthermore you must also remember that when you install a hard real time
 * handler the related interrupt is usually disabled, unless you are overtaking
 * one already owned by Linux which has been enabled by it.   Recall that if
 * have done it right, and interrupts do not show up, it is likely you have just
 * to rt_enable_irq() your irq.
 */
void rt_ack_irq (unsigned irq)
{
	_rt_enable_irq(irq);
}

void rt_end_irq (unsigned irq)
{
	_rt_end_irq(irq);
}

void rt_eoi_irq (unsigned irq)
{
	BEGIN_PIC();
	if (
#ifdef CONFIG_X86_IO_APIC
	    !IO_APIC_IRQ(irq) ||
#endif /* CONFIG_X86_IO_APIC */
	    !(irq_desc[irq].status & (IRQ_DISABLED | IRQ_INPROGRESS))) {
		hal_unlock_irq(hal_root_domain, irq);
	}
#if defined(CONFIG_X86_IO_APIC) && LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,19)
	rtai_irq_desc(irq)->eoi(irq);
#else /* !(CONFIG_X86_IO_APIC && LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,19))
*/
	rtai_irq_desc(irq)->end(irq);
#endif
	END_PIC();
}

/**
 * Install shared Linux interrupt handler.
 *
 * rt_request_linux_irq installs function @a handler as a standard Linux
 * interrupt service routine for IRQ level @a irq forcing Linux to share the IRQ
 * with other interrupt handlers, even if it does not want. The handler is
 * appended to any already existing Linux handler for the same irq and is run by
 * Linux irq as any of its handler. In this way a real time application can
 * monitor Linux interrupts handling at its will. The handler appears in
 * /proc/interrupts.
 *
 * @param handler pointer on the interrupt service routine to be installed.
 *
 * @param name is a name for /proc/interrupts.
 *
 * @param dev_id is to pass to the interrupt handler, in the same way as the
 * standard Linux irq request call.
 *
 * The interrupt service routine can be uninstalled with rt_free_linux_irq().
 *
 * @retval 0 on success.
 * @retval EINVAL if @a irq is not a valid IRQ number or handler is @c NULL.
 * @retval EBUSY if there is already a handler of interrupt @a irq.
 */
int rt_request_linux_irq (unsigned irq, void *handler, char *name, void *dev_id)
{
	unsigned long flags;
	int retval;

	if (irq >= RTAI_NR_IRQS || !handler) {
		return -EINVAL;
	}

	rtai_save_flags_and_cli(flags);
	spin_lock(&irq_desc[irq].lock);
	if (rtai_linux_irq[irq].count++ == 0 && irq_desc[irq].action) {
		rtai_linux_irq[irq].flags = irq_desc[irq].action->flags;
		irq_desc[irq].action->flags |= IRQF_SHARED;
	}
	spin_unlock(&irq_desc[irq].lock);
	rtai_restore_flags(flags);

	retval = request_irq(irq, handler, IRQF_SHARED, name, dev_id);

	return retval;
}

/**
 * Uninstall shared Linux interrupt handler.
 *
 * @param dev_id is to pass to the interrupt handler, in the same way as the
 * standard Linux irq request call.
 *
 * @param irq is the IRQ level of the interrupt handler to be freed.
 *
 * @retval 0 on success.
 * @retval EINVAL if @a irq is not a valid IRQ number.
 */
int rt_free_linux_irq (unsigned irq, void *dev_id)
{
	unsigned long flags;

	if (irq >= RTAI_NR_IRQS || rtai_linux_irq[irq].count == 0) {
		return -EINVAL;
	}

	rtai_save_flags_and_cli(flags);
	free_irq(irq,dev_id);
	spin_lock(&irq_desc[irq].lock);
	if (--rtai_linux_irq[irq].count == 0 && irq_desc[irq].action) {
		irq_desc[irq].action->flags = rtai_linux_irq[irq].flags;
	}
	spin_unlock(&irq_desc[irq].lock);
	rtai_restore_flags(flags);

	return 0;
}

/**
 * Pend an IRQ to Linux.
 *
 * rt_pend_linux_irq appends a Linux interrupt irq for processing in Linux IRQ
 * mode, i.e. with hardware interrupts fully enabled.
 *
 * @note rt_pend_linux_irq does not perform any check on @a irq.
 */
void rt_pend_linux_irq (unsigned irq)
{
	unsigned long flags;
	rtai_save_flags_and_cli(flags);
	hal_pend_uncond(irq, rtai_cpuid());
	rtai_restore_flags(flags);
}

RTAI_SYSCALL_MODE void usr_rt_pend_linux_irq (unsigned irq)
{
        unsigned long flags;
        rtai_save_flags_and_cli(flags);
        hal_pend_uncond(irq, rtai_cpuid());
        rtai_restore_flags(flags);
}

/**
 * Install a system request handler
 *
 * rt_request_srq installs a two way RTAI system request (srq) by assigning
 * @a u_handler, a function to be used when a user calls srq from user space,
 * and @a k_handler, the function to be called in kernel space following its
 * activation by a call to rt_pend_linux_srq(). @a k_handler is in practice
 * used to request a service from the kernel. In fact Linux system requests
 * cannot be used safely from RTAI so you can setup a handler that receives real
 * time requests and safely executes them when Linux is running.
 *
 * @param u_handler can be used to effectively enter kernel space without the
 * overhead and clumsiness of standard Unix/Linux protocols.   This is very
 * flexible service that allows you to personalize your use of  RTAI.
 *
 * @return the number of the assigned system request on success.
 * @retval EINVAL if @a k_handler is @c NULL.
 * @retval EBUSY if no free srq slot is available.
 */
int rt_request_srq (unsigned label, void (*k_handler)(void), long long (*u_handler)(unsigned long))
{
	unsigned long flags;
	int srq;

	if (k_handler == NULL) {
		return -EINVAL;
	}

	rtai_save_flags_and_cli(flags);
	if (rtai_sysreq_map != ~0) {
		set_bit(srq = ffz(rtai_sysreq_map), &rtai_sysreq_map);
		rtai_sysreq_table[srq].k_handler = k_handler;
		rtai_sysreq_table[srq].u_handler = u_handler;
		rtai_sysreq_table[srq].label = label;
	} else {
		srq = -EBUSY;
	}
	rtai_restore_flags(flags);

	return srq;
}

/**
 * Uninstall a system request handler
 *
 * rt_free_srq uninstalls the specified system call @a srq, returned by
 * installing the related handler with a previous call to rt_request_srq().
 *
 * @retval EINVAL if @a srq is invalid.
 */
int rt_free_srq (unsigned srq)
{
	return  (srq < 2 || srq >= RTAI_NR_SRQS || !test_and_clear_bit(srq, &rtai_sysreq_map)) ? -EINVAL : 0;
}

/**
 * Append a Linux IRQ.
 *
 * rt_pend_linux_srq appends a system call request srq to be used as a service
 * request to the Linux kernel.
 *
 * @param srq is the value returned by rt_request_srq.
 *
 * @note rt_pend_linux_srq does not perform any check on irq.
 */
void rt_pend_linux_srq (unsigned srq)
{
	if (srq > 0 && srq < RTAI_NR_SRQS) {
		unsigned long flags;
		set_bit(srq, &rtai_sysreq_pending);
		rtai_save_flags_and_cli(flags);
		hal_pend_uncond(rtai_sysreq_virq, rtai_cpuid());
		rtai_restore_flags(flags);
	}
}

#ifdef CONFIG_X86_LOCAL_APIC

irqreturn_t rtai_broadcast_to_local_timers (int irq, void *dev_id, struct pt_regs *regs)
{
	unsigned long flags;

	rtai_hw_save_flags_and_cli(flags);
#ifdef CONFIG_SMP
	apic_wait_icr_idle();
//	apic_write_around(APIC_ICR,APIC_DM_FIXED|APIC_DEST_ALLINC|LOCAL_TIMER_VECTOR);
	apic_write_around(APIC_ICR,APIC_DM_FIXED|APIC_DEST_ALLBUT|LOCAL_TIMER_VECTOR);
#endif
	hal_pend_uncond(LOCAL_TIMER_IPI, rtai_cpuid());
	rtai_hw_restore_flags(flags);

	return RTAI_LINUX_IRQ_HANDLED;
} 

#ifdef CONFIG_GENERIC_CLOCKEVENTS

static inline int REQUEST_LINUX_IRQ_BROADCAST_TO_APIC_TIMERS(void)
{
	return 0;
}

#define FREE_LINUX_IRQ_BROADCAST_TO_APIC_TIMERS();

#else

#define REQUEST_LINUX_IRQ_BROADCAST_TO_APIC_TIMERS()  rt_request_linux_irq(RTAI_TIMER_8254_IRQ, &rtai_broadcast_to_local_timers, "rtai_broadcast", &rtai_broadcast_to_local_timers)

#define FREE_LINUX_IRQ_BROADCAST_TO_APIC_TIMERS()     rt_free_linux_irq(RTAI_TIMER_8254_IRQ, &rtai_broadcast_to_local_timers)

#endif

#else

irqreturn_t rtai_broadcast_to_local_timers (int irq, void *dev_id, struct pt_regs *regs)
{
	return RTAI_LINUX_IRQ_HANDLED;
} 

#define REQUEST_LINUX_IRQ_BROADCAST_TO_APIC_TIMERS()  0

#define FREE_LINUX_IRQ_BROADCAST_TO_APIC_TIMERS();

#endif

#ifdef CONFIG_RTAI_SCHED_ISR_LOCK
#define RTAI_SCHED_ISR_LOCK() \
	do { \
		if (!rt_scheduling[cpuid = rtai_cpuid()].locked++) { \
			rt_scheduling[cpuid].rqsted = 0; \
		} \
	} while (0)
#define RTAI_SCHED_ISR_UNLOCK() \
	do { \
		if (rt_scheduling[cpuid].locked && !(--rt_scheduling[cpuid].locked)) { \
			if (rt_scheduling[cpuid].rqsted > 0 && rtai_isr_hook) { \
				rtai_isr_hook(cpuid); \
        		} \
		} \
	} while (0)
#else  /* !CONFIG_RTAI_SCHED_ISR_LOCK */
#define RTAI_SCHED_ISR_LOCK() \
	do { cpuid = rtai_cpuid(); } while (0)
#define RTAI_SCHED_ISR_UNLOCK() \
	do {                       } while (0)
#endif /* CONFIG_RTAI_SCHED_ISR_LOCK */

#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,9)
#define HAL_TICK_REGS hal_tick_regs[cpuid]
#else
#define HAL_TICK_REGS hal_tick_regs
#endif

#ifdef LOCKED_LINUX_IN_IRQ_HANDLER
#define HAL_LOCK_LINUX()  do { sflags = rt_save_switch_to_real_time(cpuid); } while (0)
#define HAL_UNLOCK_LINUX()  do { rtai_cli(); rt_restore_switch_to_linux(sflags, cpuid); } while (0)
#else
#define HAL_LOCK_LINUX()  do { sflags = xchg(ROOT_STATUS_ADR(cpuid), (1 << IPIPE_STALL_FLAG)); } while (0)
#define HAL_UNLOCK_LINUX()  do { rtai_cli(); ROOT_STATUS_VAL(cpuid) = sflags; } while (0)
#endif

#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,18)
#define __ALIGN_STR ".align 16,0x90"
#endif

#ifndef STR
#define __STR(x) #x
#define STR(x) __STR(x)
#endif

#ifndef SYMBOL_NAME_STR
#define SYMBOL_NAME_STR(X) #X
#endif

#if CONFIG_RTAI_DONT_DISPATCH_CORE_IRQS

#define SET_INTR_GATE(vector, handler, save) \
	do { save = rtai_set_gate_vector(vector, GATE_INTERRUPT, 0, handler); } while (0)
#define RESET_INTR_GATE(vector, save) \
	do { rtai_reset_gate_vector(vector, save); } while (0)

#ifdef CONFIG_SMP
int _rtai_sched_on_ipi_handler(void)
{
        unsigned long cpuid;
        unsigned long sflags;

        RTAI_SCHED_ISR_LOCK();
        HAL_LOCK_LINUX();
        __ack_APIC_irq();
//      rtai_realtime_irq[SCHED_IPI].irq_ack(SCHED_IPI);
//      hal_root_domain->irqs[SCHED_IPI].acknowledge(SCHED_IPI);
        ((void (*)(void))rtai_realtime_irq[SCHED_IPI].handler)();
        HAL_UNLOCK_LINUX();
        RTAI_SCHED_ISR_UNLOCK();
	if (!test_bit(IPIPE_STALL_FLAG, ROOT_STATUS_ADR(cpuid))) {
                rtai_sti();
                hal_fast_flush_pipeline(cpuid);
#if defined(CONFIG_SMP) &&  LINUX_VERSION_CODE < KERNEL_VERSION(2,6,19)
                __set_bit(IPIPE_STALL_FLAG, ROOT_STATUS_ADR(rtai_cpuid()]);
#endif
                return 1;
        }
        return 0;
}

void rtai_sched_on_ipi_handler (void);
	__asm__ ( \
        "\n" __ALIGN_STR"\n\t" \
        SYMBOL_NAME_STR(rtai_sched_on_ipi_handler) ":\n\t" \
	"\n .p2align\n" \
	"pushq $0\n" \
	"cld\n" \
	"sub    $0x48,%rsp\n" \
	"mov    %rdi,0x40(%rsp)\n" \
	"mov    %rsi,0x38(%rsp)\n" \
	"mov    %rdx,0x30(%rsp)\n" \
	"mov    %rcx,0x28(%rsp)\n" \
	"mov    %rax,0x20(%rsp)\n" \
	"mov    %r8,0x18(%rsp)\n" \
	"mov    %r9,0x10(%rsp)\n" \
	"mov    %r10,0x8(%rsp)\n" \
	"mov    %r11,(%rsp)\n" \
	"lea    0xffffffffffffffd0(%rsp),%rdi\n" \
	"testl  $0x3,0x88(%rdi)\n" \
	"je 1f\n" \
	"swapgs\n" \
	"1:addl   $0x1,%gs:0x30\n" \
	"mov    %gs:0x38,%rax\n" \
	"cmove  %rax,%rsp\n" \
	"push   %rdi\n" \
        "call "SYMBOL_NAME_STR(_rtai_sched_on_ipi_handler)"\n\t" \
	"test   %rax,%rax\n" \
	"jnz  ret_from_intr\n" \
	"pop    %rdi\n" \
	"cli\n" \
	"subl   $0x1,%gs:0x30\n" \
	"lea    0x30(%rdi),%rsp\n" \
	"mov    %gs:0x18,%rcx\n" \
	"sub    $0x1fd8,%rcx\n" \
	"testl  $0x3,0x58(%rsp)\n" \
	"je 2f\n" \
	"swapgs\n" \
	"2:mov    (%rsp),%r11\n" \
	"mov    0x8(%rsp),%r10\n" \
	"mov    0x10(%rsp),%r9\n" \
	"mov    0x18(%rsp),%r8\n" \
	"mov    0x20(%rsp),%rax\n" \
	"mov    0x28(%rsp),%rcx\n" \
	"mov    0x30(%rsp),%rdx\n" \
	"mov    0x38(%rsp),%rsi\n" \
	"mov    0x40(%rsp),%rdi\n" \
	"add    $0x50,%rsp\n" \
	"iretq");


static struct gate_struct rtai_sched_on_ipi_sysvec;

void rt_set_sched_ipi_gate(void)
{
	SET_INTR_GATE(SCHED_VECTOR, rtai_sched_on_ipi_handler, rtai_sched_on_ipi_sysvec);
}

void rt_reset_sched_ipi_gate(void)
{
	RESET_INTR_GATE(SCHED_VECTOR, rtai_sched_on_ipi_sysvec);
}
#endif

int _rtai_apic_timer_handler(void)
{
        unsigned long cpuid;
        unsigned long sflags;

	RTAI_SCHED_ISR_LOCK();
        HAL_LOCK_LINUX();
        __ack_APIC_irq();
//      rtai_realtime_irq[RTAI_APIC_TIMER_IPI].irq_ack(RTAI_APIC_TIMER_IPI);
//      hal_root_domain->irqs[RTAI_APIC_TIMER_IPI].acknowledge(RTAI_APIC_TIMER_IPI);
        ((void (*)(void))rtai_realtime_irq[RTAI_APIC_TIMER_IPI].handler)();
        HAL_UNLOCK_LINUX();
	RTAI_SCHED_ISR_UNLOCK();
	if (!test_bit(IPIPE_STALL_FLAG, ROOT_STATUS_ADR(cpuid))) {
                rtai_sti();
                hal_fast_flush_pipeline(cpuid);
#if defined(CONFIG_SMP) &&  LINUX_VERSION_CODE < KERNEL_VERSION(2,6,19)
                __set_bit(IPIPE_STALL_FLAG, ROOT_STATUS_ADR(rtai_cpuid());
#endif
                return 1;
        }
        return 0;
}

void rtai_apic_timer_handler (void);
	__asm__ ( \
	"\n" __ALIGN_STR"\n\t" \
	SYMBOL_NAME_STR(rtai_apic_timer_handler) ":\n\t" \
	"\n .p2align\n" \
	"pushq $0\n" \
	"cld\n" \
	"sub    $0x48,%rsp\n" \
	"mov    %rdi,0x40(%rsp)\n" \
	"mov    %rsi,0x38(%rsp)\n" \
	"mov    %rdx,0x30(%rsp)\n" \
	"mov    %rcx,0x28(%rsp)\n" \
	"mov    %rax,0x20(%rsp)\n" \
	"mov    %r8,0x18(%rsp)\n" \
	"mov    %r9,0x10(%rsp)\n" \
	"mov    %r10,0x8(%rsp)\n" \
	"mov    %r11,(%rsp)\n" \
	"lea    0xffffffffffffffd0(%rsp),%rdi\n" \
	"testl  $0x3,0x88(%rdi)\n" \
	"je 1f\n" \
	"swapgs\n" \
	"1:addl   $0x1,%gs:0x30\n" \
	"mov    %gs:0x38,%rax\n" \
	"cmove  %rax,%rsp\n" \
	"push   %rdi\n" \
	"call "SYMBOL_NAME_STR(_rtai_apic_timer_handler)"\n\t" \
	"test   %rax,%rax\n" \
	"jnz  ret_from_intr\n" \
	"pop    %rdi\n" \
	"cli\n" \
	"subl   $0x1,%gs:0x30\n" \
	"lea    0x30(%rdi),%rsp\n" \
	"mov    %gs:0x18,%rcx\n" \
	"sub    $0x1fd8,%rcx\n" \
	"testl  $0x3,0x58(%rsp)\n" \
	"je 2f\n" \
	"swapgs\n" \
	"2:mov    (%rsp),%r11\n" \
	"mov    0x8(%rsp),%r10\n" \
	"mov    0x10(%rsp),%r9\n" \
	"mov    0x18(%rsp),%r8\n" \
	"mov    0x20(%rsp),%rax\n" \
	"mov    0x28(%rsp),%rcx\n" \
	"mov    0x30(%rsp),%rdx\n" \
	"mov    0x38(%rsp),%rsi\n" \
	"mov    0x40(%rsp),%rdi\n" \
	"add    $0x50,%rsp\n" \
	"iretq");

static struct gate_struct rtai_apic_timer_sysvec;

/* this can be a prototype for a handler pending something for Linux */
int _rtai_8254_timer_handler(struct pt_regs *regs)
{
        unsigned long cpuid;
        unsigned long sflags;

        RTAI_SCHED_ISR_LOCK();
        HAL_LOCK_LINUX();
        rtai_realtime_irq[RTAI_TIMER_8254_IRQ].irq_ack(RTAI_TIMER_8254_IRQ);
        ((void (*)(void))rtai_realtime_irq[RTAI_TIMER_8254_IRQ].handler)();
        HAL_UNLOCK_LINUX();
        RTAI_SCHED_ISR_UNLOCK();
	if (!test_bit(IPIPE_STALL_FLAG, ROOT_STATUS_ADR(cpuid))) {
		rtai_sti();
#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
/* specific for the Linux tick, do not cre in a generic handler */
		HAL_TICK_REGS.eflags = regs->eflags;
		HAL_TICK_REGS.rip    = regs->rip;
		HAL_TICK_REGS.cs     = regs->cs;
		HAL_TICK_REGS.rsp    = regs->rsp;
#if defined(CONFIG_SMP) && defined(CONFIG_FRAME_POINTER) && LINUX_VERSION_CODE < KERNEL_VERSION(2,6,19)
		HAL_TICK_REGS.rbp    = regs->rbp;
#endif /* CONFIG_SMP && CONFIG_FRAME_POINTER */
#else
		__raw_get_cpu_var(__ipipe_tick_regs).rip    = regs->rip;
		__raw_get_cpu_var(__ipipe_tick_regs).cs     = regs->cs;
		__raw_get_cpu_var(__ipipe_tick_regs).eflags = regs->eflags;
		__raw_get_cpu_var(__ipipe_tick_regs).rbp    = regs->rbp;
		__raw_get_cpu_var(__ipipe_tick_regs).rsp    = regs->rsp;
		__raw_get_cpu_var(__ipipe_tick_regs).ss     = regs->ss;
#endif
		hal_fast_flush_pipeline(cpuid);
#if defined(CONFIG_SMP) && LINUX_VERSION_CODE < KERNEL_VERSION(2,6,19)
                __set_bit(IPIPE_STALL_FLAG, ROOT_STATUS_ADR(rtai_cpuid()));
#endif
                return 1;
        }
	return 0;
}

void rtai_8254_timer_handler (void);
	__asm__ ( \
        "\n" __ALIGN_STR"\n\t" \
        SYMBOL_NAME_STR(rtai_8254_timer_handler) ":\n\t" \
	"\n .p2align\n" \
	"pushq $0\n" \
	"cld\n" \
	"sub    $0x48,%rsp\n" \
	"mov    %rdi,0x40(%rsp)\n" \
	"mov    %rsi,0x38(%rsp)\n" \
	"mov    %rdx,0x30(%rsp)\n" \
	"mov    %rcx,0x28(%rsp)\n" \
	"mov    %rax,0x20(%rsp)\n" \
	"mov    %r8,0x18(%rsp)\n" \
	"mov    %r9,0x10(%rsp)\n" \
	"mov    %r10,0x8(%rsp)\n" \
	"mov    %r11,(%rsp)\n" \
	"lea    0xffffffffffffffd0(%rsp),%rdi\n" \
	"testl  $0x3,0x88(%rdi)\n" \
	"je 1f\n" \
	"swapgs\n" \
	"1:addl   $0x1,%gs:0x30\n" \
	"mov    %gs:0x38,%rax\n" \
	"cmove  %rax,%rsp\n" \
	"push   %rdi\n" \
	"call "SYMBOL_NAME_STR(_rtai_8254_timer_handler)"\n\t" \
	"test   %rax,%rax\n" \
	"jnz  ret_from_intr\n" \
	"pop    %rdi\n" \
	"cli\n" \
	"subl   $0x1,%gs:0x30\n" \
	"lea    0x30(%rdi),%rsp\n" \
	"mov    %gs:0x18,%rcx\n" \
	"sub    $0x1fd8,%rcx\n" \
	"testl  $0x3,0x58(%rsp)\n" \
	"je 2f\n" \
	"swapgs\n" \
	"2:mov    (%rsp),%r11\n" \
	"mov    0x8(%rsp),%r10\n" \
	"mov    0x10(%rsp),%r9\n" \
	"mov    0x18(%rsp),%r8\n" \
	"mov    0x20(%rsp),%rax\n" \
	"mov    0x28(%rsp),%rcx\n" \
	"mov    0x30(%rsp),%rdx\n" \
	"mov    0x38(%rsp),%rsi\n" \
	"mov    0x40(%rsp),%rdi\n" \
	"add    $0x50,%rsp\n" \
	"iretq");

static struct gate_struct rtai_8254_timer_sysvec;

#else

#define SET_INTR_GATE(vector, handler, save) 
#define RESET_INTR_GATE(vector, save)

void rt_set_sched_ipi_gate(void) { }
void rt_reset_sched_ipi_gate(void) { }

#endif

#ifdef CONFIG_SMP

static unsigned long rtai_old_irq_affinity[IPIPE_NR_XIRQS];
static int rtai_orig_irq_affinity[IPIPE_NR_XIRQS];

static spinlock_t rtai_iset_lock = SPIN_LOCK_UNLOCKED;

static long long rtai_timers_sync_time;

static struct apic_timer_setup_data rtai_timer_mode[RTAI_NR_CPUS];

static void rtai_critical_sync (void)
{
	struct apic_timer_setup_data *p;

	switch (rtai_sync_level) {
		case 1: {
			p = &rtai_timer_mode[rtai_cpuid()];
			while (rtai_rdtsc() < rtai_timers_sync_time);
			if (p->mode) {
				rtai_setup_periodic_apic(p->count, RTAI_APIC_TIMER_VECTOR);
			} else {
				rtai_setup_oneshot_apic(p->count, RTAI_APIC_TIMER_VECTOR);
			}
			break;
		}
		case 2: {
			rtai_setup_oneshot_apic(0, RTAI_APIC_TIMER_VECTOR);
			break;
		}
		case 3: {
			rtai_setup_periodic_apic(RTAI_APIC_ICOUNT, LOCAL_TIMER_VECTOR);
			break;
		}
	}
}

/**
 * Install a local APICs timer interrupt handler
 *
 * rt_request_apic_timers requests local APICs timers and defines the mode and
 * count to be used for each local APIC timer. Modes and counts can be chosen
 * arbitrarily for each local APIC timer.
 *
 * @param apic_timer_data is a pointer to a vector of structures
 * @code struct apic_timer_setup_data { int mode, count; }
 * @endcode sized with the number of CPUs available.
 *
 * Such a structure defines:
 * - mode: 0 for a oneshot timing, 1 for a periodic timing.
 * - count: is the period in nanoseconds you want to use on the corresponding
 * timer, not used for oneshot timers.  It is in nanoseconds to ease its
 * programming when different values are used by each timer, so that you do not
 * have to care converting it from the CPU on which you are calling this
 * function.
 *
 * The start of the timing should be reasonably synchronized.   You should call
 * this function with due care and only when you want to manage the related
 * interrupts in your own handler.   For using local APIC timers in pacing real
 * time tasks use the usual rt_start_timer(), which under the MUP scheduler sets
 * the same timer policy on all the local APIC timers, or start_rt_apic_timers()
 * that allows you to use @c struct @c apic_timer_setup_data directly.
 */
void rt_request_apic_timers (void (*handler)(void), struct apic_timer_setup_data *tmdata)
{
	volatile struct rt_times *rtimes;
	struct apic_timer_setup_data *p;
	unsigned long flags;
	int cpuid;

	TRACE_RTAI_TIMER(TRACE_RTAI_EV_TIMER_REQUEST_APIC,handler,0);

	flags = rtai_critical_enter(rtai_critical_sync);
	rtai_sync_level = 1;
	rtai_timers_sync_time = rtai_rdtsc() + rtai_imuldiv(LATCH, rtai_tunables.cpu_freq, RTAI_FREQ_8254);
	for (cpuid = 0; cpuid < RTAI_NR_CPUS; cpuid++) {
		p = &rtai_timer_mode[cpuid];
		*p = tmdata[cpuid];
		rtimes = &rt_smp_times[cpuid];
		if (p->mode) {
			rtimes->linux_tick = RTAI_APIC_ICOUNT;
			rtimes->tick_time = rtai_llimd(rtai_timers_sync_time, RTAI_FREQ_APIC, rtai_tunables.cpu_freq);
			rtimes->periodic_tick = rtai_imuldiv(p->count, RTAI_FREQ_APIC, 1000000000);
			p->count = rtimes->periodic_tick;
		} else {
			rtimes->linux_tick = rtai_imuldiv(LATCH, rtai_tunables.cpu_freq, RTAI_FREQ_8254);
			rtimes->tick_time = rtai_timers_sync_time;
			rtimes->periodic_tick = rtimes->linux_tick;
			p->count = RTAI_APIC_ICOUNT;
		}
		rtimes->intr_time = rtimes->tick_time + rtimes->periodic_tick;
		rtimes->linux_time = rtimes->tick_time + rtimes->linux_tick;
	}

	p = &rtai_timer_mode[rtai_cpuid()];
	while (rtai_rdtsc() < rtai_timers_sync_time) ;

	if (p->mode) {
		rtai_setup_periodic_apic(p->count,RTAI_APIC_TIMER_VECTOR);
	} else {
		rtai_setup_oneshot_apic(p->count,RTAI_APIC_TIMER_VECTOR);
	}

	rt_release_irq(RTAI_APIC_TIMER_IPI);
	rt_request_irq(RTAI_APIC_TIMER_IPI, (rt_irq_handler_t)handler, NULL, 0);
	SET_INTR_GATE(RTAI_APIC_TIMER_VECTOR, rtai_apic_timer_handler, rtai_apic_timer_sysvec);

	REQUEST_LINUX_IRQ_BROADCAST_TO_APIC_TIMERS();

	for (cpuid = 0; cpuid < RTAI_NR_CPUS; cpuid++) {
		p = &tmdata[cpuid];
		if (p->mode) {
			p->count = rtai_imuldiv(p->count,RTAI_FREQ_APIC,1000000000);
		} else {
			p->count = rtai_imuldiv(p->count,rtai_tunables.cpu_freq,1000000000);
		}
	}

	rtai_request_tickdev();
	rtai_critical_exit(flags);
}

/**
 * Uninstall a local APICs timer interrupt handler
 */
void rt_free_apic_timers(void)
{
	unsigned long flags;

	TRACE_RTAI_TIMER(TRACE_RTAI_EV_TIMER_APIC_FREE,0,0);

	FREE_LINUX_IRQ_BROADCAST_TO_APIC_TIMERS();
	flags = rtai_critical_enter(rtai_critical_sync);
	rtai_release_tickdev();
	rtai_sync_level = 3;
	rtai_setup_periodic_apic(RTAI_APIC_ICOUNT,LOCAL_TIMER_VECTOR);
	RESET_INTR_GATE(RTAI_APIC_TIMER_VECTOR, rtai_apic_timer_sysvec);
	rt_release_irq(RTAI_APIC_TIMER_IPI);
	rtai_critical_exit(flags);
}

/**
 * Set IRQ->CPU assignment
 *
 * rt_assign_irq_to_cpu forces the assignment of the external interrupt @a irq
 * to the CPUs of an assigned mask.
 *
 * @the mask of the interrupts routing before its call.
 * @-EINVAL if @a irq is not a valid IRQ number or some internal data
 * inconsistency is found.
 *
 * @note This functions has effect only on multiprocessors systems.
 * @note With Linux 2.4.xx such a service has finally been made available
 * natively within the raw kernel. With such Linux releases
 * rt_reset_irq_to_sym_mode() resets the original Linux delivery mode, or
 * deliver affinity as they call it. So be warned that such a name is kept
 * mainly for compatibility reasons, as for such a kernel the reset operation
 * does not necessarily implies a symmetric external interrupt delivery.
 */
int rt_assign_irq_to_cpu (int irq, unsigned long cpumask)
{
	if (irq >= IPIPE_NR_XIRQS) {
		return -EINVAL;
	} else {
		unsigned long oldmask, flags;

		rtai_save_flags_and_cli(flags);
		spin_lock(&rtai_iset_lock);
		if ((oldmask = CPUMASK(hal_set_irq_affinity(irq, CPUMASK_T(cpumask))))) {
			rtai_old_irq_affinity[irq] = oldmask;
		}
		spin_unlock(&rtai_iset_lock);
		rtai_restore_flags(flags);

		return oldmask;
	}
}

/**
 * reset IRQ->CPU assignment
 *
 * rt_reset_irq_to_sym_mode resets the interrupt irq to the symmetric interrupts
 * management, whatever that means, existing before the very first use of RTAI 
 * rt_assign_irq_to_cpu. This function applies to external interrupts only.
 *
 * @the mask of the interrupts routing before its call.
 * @-EINVAL if @a irq is not a valid IRQ number or some internal data
 * inconsistency is found.
 *
 * @note This function has effect only on multiprocessors systems.
 * @note With Linux 2.4.xx such a service has finally been made available
 * natively within the raw kernel. With such Linux releases
 * rt_reset_irq_to_sym_mode() resets the original Linux delivery mode, or
 * deliver affinity as they call it. So be warned that such a name is kept
 * mainly for compatibility reasons, as for such a kernel the reset operation
 * does not necessarily implies a symmetric external interrupt delivery.
 */
int rt_reset_irq_to_sym_mode (int irq)
{
	unsigned long oldmask, flags;

	if (irq >= IPIPE_NR_XIRQS) {
		return -EINVAL;
	} else {
		rtai_save_flags_and_cli(flags);
		spin_lock(&rtai_iset_lock);
		if (rtai_old_irq_affinity[irq] == 0) {
			spin_unlock(&rtai_iset_lock);
			rtai_restore_flags(flags);
			return -EINVAL;
		}
		oldmask = CPUMASK(hal_set_irq_affinity(irq, CPUMASK_T(0)));
		if (rtai_old_irq_affinity[irq]) {
	        	hal_set_irq_affinity(irq, CPUMASK_T(rtai_old_irq_affinity[irq]));
	        	rtai_old_irq_affinity[irq] = 0;
        	}
		spin_unlock(&rtai_iset_lock);
		rtai_restore_flags(flags);

		return oldmask;
	}
}

#else  /* !CONFIG_SMP */

#define rtai_critical_sync NULL

void rt_request_apic_timers (void (*handler)(void), struct apic_timer_setup_data *tmdata)
{
	return;
}

void rt_free_apic_timers(void)
{
	rt_free_timer();
}

int rt_assign_irq_to_cpu (int irq, unsigned long cpus_mask)
{
	return 0;
}

int rt_reset_irq_to_sym_mode (int irq)
{
	return 0;
}

#endif /* CONFIG_SMP */

/**
 * Install a timer interrupt handler.
 *
 * rt_request_timer requests a timer of period tick ticks, and installs the
 * routine @a handler as a real time interrupt service routine for the timer.
 *
 * Set @a tick to 0 for oneshot mode (in oneshot mode it is not used).
 * If @a apic has a nonzero value the local APIC timer is used.   Otherwise
 * timing is based on the 8254.
 *
 */
static int unsigned long used_apic;

int rt_request_timer (void (*handler)(void), unsigned tick, int use_apic)
{
	unsigned long flags;
	int retval;

	TRACE_RTAI_TIMER(TRACE_RTAI_EV_TIMER_REQUEST,handler,tick);

	used_apic = use_apic;
	rtai_save_flags_and_cli(flags);
	rt_times.tick_time = rtai_rdtsc();
    	if (tick > 0) {
		rt_times.linux_tick = use_apic ? RTAI_APIC_ICOUNT : LATCH;
		rt_times.tick_time = ((RTIME)rt_times.linux_tick)*(jiffies + 1);
		rt_times.intr_time = rt_times.tick_time + tick;
		rt_times.linux_time = rt_times.tick_time + rt_times.linux_tick;
		rt_times.periodic_tick = tick;

		if (use_apic) {
			rt_release_irq(RTAI_APIC_TIMER_IPI);
			rt_request_irq(RTAI_APIC_TIMER_IPI, (rt_irq_handler_t)handler, NULL, 0);
			SET_INTR_GATE(RTAI_APIC_TIMER_VECTOR, rtai_apic_timer_handler, rtai_apic_timer_sysvec);
			rtai_setup_periodic_apic(tick,RTAI_APIC_TIMER_VECTOR);
			retval = REQUEST_LINUX_IRQ_BROADCAST_TO_APIC_TIMERS();
		} else {
			outb(0x34, 0x43);
			outb(tick & 0xff, 0x40);
			outb(tick >> 8, 0x40);
			rt_release_irq(RTAI_TIMER_8254_IRQ);
		    	retval = rt_request_irq(RTAI_TIMER_8254_IRQ, (rt_irq_handler_t)handler, NULL, 0);
			SET_INTR_GATE(ext_irq_vector(RTAI_TIMER_8254_IRQ), rtai_8254_timer_handler, rtai_8254_timer_sysvec);
		}
	} else {
		rt_times.linux_tick = rtai_imuldiv(LATCH,rtai_tunables.cpu_freq,RTAI_FREQ_8254);
		rt_times.intr_time = rt_times.tick_time + rt_times.linux_tick;
		rt_times.linux_time = rt_times.tick_time + rt_times.linux_tick;
		rt_times.periodic_tick = rt_times.linux_tick;

		if (use_apic) {
			rt_release_irq(RTAI_APIC_TIMER_IPI);
			rt_request_irq(RTAI_APIC_TIMER_IPI, (rt_irq_handler_t)handler, NULL, 0);
			SET_INTR_GATE(RTAI_APIC_TIMER_VECTOR, rtai_apic_timer_handler, rtai_apic_timer_sysvec);
			rtai_setup_oneshot_apic(RTAI_APIC_ICOUNT,RTAI_APIC_TIMER_VECTOR);
    			retval = REQUEST_LINUX_IRQ_BROADCAST_TO_APIC_TIMERS();
		} else {
			outb(0x30, 0x43);
			outb(LATCH & 0xff, 0x40);
			outb(LATCH >> 8, 0x40);
			rt_release_irq(RTAI_TIMER_8254_IRQ);
			retval = rt_request_irq(RTAI_TIMER_8254_IRQ, (rt_irq_handler_t)handler, NULL, 0);
			SET_INTR_GATE(ext_irq_vector(RTAI_TIMER_8254_IRQ), rtai_8254_timer_handler, rtai_8254_timer_sysvec);
		}
	}
	rtai_request_tickdev();
	rtai_restore_flags(flags);
	return retval;
}

/**
 * Uninstall a timer interrupt handler.
 *
 * rt_free_timer uninstalls a timer previously set by rt_request_timer().
 */
void rt_free_timer (void)
{
	unsigned long flags;

	TRACE_RTAI_TIMER(TRACE_RTAI_EV_TIMER_FREE,0,0);

	rtai_save_flags_and_cli(flags);
	rtai_release_tickdev();
	if (used_apic) {
		FREE_LINUX_IRQ_BROADCAST_TO_APIC_TIMERS();
		rtai_setup_periodic_apic(RTAI_APIC_ICOUNT, LOCAL_TIMER_VECTOR);
		RESET_INTR_GATE(RTAI_APIC_TIMER_VECTOR, rtai_apic_timer_sysvec);
		rt_release_irq(RTAI_APIC_TIMER_IPI);
		used_apic = 0;
	} else {
		outb(0x34, 0x43);
		outb(LATCH & 0xff, 0x40);
		outb(LATCH >> 8,0x40);
		if (!rt_release_irq(RTAI_TIMER_8254_IRQ)) {
			RESET_INTR_GATE(ext_irq_vector(RTAI_TIMER_8254_IRQ), rtai_8254_timer_sysvec);
		}
	}
	rtai_restore_flags(flags);
}

RT_TRAP_HANDLER rt_set_trap_handler (RT_TRAP_HANDLER handler)
{
	return (RT_TRAP_HANDLER)xchg(&rtai_trap_handler, handler);
}

RTIME rd_8254_ts (void)
{
	unsigned long flags;
	int inc, c2;
	RTIME t;

	rtai_hw_save_flags_and_cli(flags);
	outb(0xD8, 0x43);
	c2 = inb(0x42);
	inc = rtai_last_8254_counter2 - (c2 |= (inb(0x42) << 8));
	rtai_last_8254_counter2 = c2;
	t = (rtai_ts_8254 += (inc > 0 ? inc : inc + RTAI_COUNTER_2_LATCH));
	rtai_hw_restore_flags(flags);

	return t;
}

void rt_setup_8254_tsc (void)
{
	unsigned long flags;
	int c;

	flags = rtai_critical_enter(NULL);
	outb_p(0x00, 0x43);
	c = inb_p(0x40);
	c |= inb_p(0x40) << 8;
	outb_p(0xB4, 0x43);
	outb_p(RTAI_COUNTER_2_LATCH & 0xff, 0x42);
	outb_p(RTAI_COUNTER_2_LATCH >> 8, 0x42);
	rtai_ts_8254 = c + ((RTIME)LATCH)*jiffies;
	rtai_last_8254_counter2 = 0; 
	outb_p((inb_p(0x61) & 0xFD) | 1, 0x61);
	rtai_critical_exit(flags);
}

static int rtai_hirq_dispatcher(struct pt_regs *regs)
{
	unsigned long cpuid, irq;

#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,19)
        if (rtai_realtime_irq[irq = regs->orig_rax & 0xFF].handler) {
#else
        unsigned long vector = ~regs->orig_rax;
        if (rtai_realtime_irq[irq = vector >= FIRST_SYSTEM_VECTOR ? ipipe_apic_vector_irq(vector) : __get_cpu_var(vector_irq)[vector]].handler) {
#endif
                unsigned long sflags;

#ifdef RTAI_SYSCALLS_USE_HIRQ_DISPATCHER
		if (irq == RTAI_SYS_IRQ) {
			int retval;
			asmlinkage int rtai_syscall_dispatcher(struct pt_regs *);
			rtai_sti();
			retval = rtai_syscall_dispatcher(regs);
			rtai_cli();
			return retval;
		}
#endif

                RTAI_SCHED_ISR_LOCK();
                HAL_LOCK_LINUX();
                rtai_realtime_irq[irq].irq_ack(irq); mb();
//              hal_root_domain->irqs[irq].acknowledge(irq); mb();
		if (rtai_realtime_irq[irq].retmode && rtai_realtime_irq[irq].handler(irq, rtai_realtime_irq[irq].cookie)) {
                        HAL_UNLOCK_LINUX();
                        RTAI_SCHED_ISR_UNLOCK();
			return 0;
                } else {
			rtai_realtime_irq[irq].handler(irq, rtai_realtime_irq[irq].cookie);
                        HAL_UNLOCK_LINUX();
                        RTAI_SCHED_ISR_UNLOCK();
			if (test_bit(IPIPE_STALL_FLAG, ROOT_STATUS_ADR(cpuid))) {
				return 0;
			}
		}
	} else {
		unsigned long lflags;
                cpuid = rtai_cpuid();
                lflags = xchg(ROOT_STATUS_ADR(cpuid), (1 << IPIPE_STALL_FLAG));
		rtai_realtime_irq[irq].irq_ack(irq); mb();
//		hal_root_domain->irqs[irq].acknowledge(irq); mb();
		hal_pend_uncond(irq, cpuid);
		ROOT_STATUS_VAL(cpuid) = lflags;
		if (test_bit(IPIPE_STALL_FLAG, &lflags)) {
			return 0;
		}
	}
	rtai_sti();
	if (irq == hal_tick_irq) {
#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
		HAL_TICK_REGS.eflags = regs->eflags;
		HAL_TICK_REGS.rip    = regs->rip;
		HAL_TICK_REGS.cs     = regs->cs;
		HAL_TICK_REGS.rsp    = regs->rsp;
#if defined(CONFIG_SMP) && defined(CONFIG_FRAME_POINTER) && LINUX_VERSION_CODE < KERNEL_VERSION(2,6,19)
		HAL_TICK_REGS.rbp    = regs->rbp;
#endif /* CONFIG_SMP && CONFIG_FRAME_POINTER */
#else
		__raw_get_cpu_var(__ipipe_tick_regs).rip    = regs->rip;
		__raw_get_cpu_var(__ipipe_tick_regs).cs     = regs->cs;
		__raw_get_cpu_var(__ipipe_tick_regs).eflags = regs->eflags;
		__raw_get_cpu_var(__ipipe_tick_regs).rbp    = regs->rbp;
		__raw_get_cpu_var(__ipipe_tick_regs).rsp    = regs->rsp;
		__raw_get_cpu_var(__ipipe_tick_regs).ss     = regs->ss;
#endif
	}
	hal_fast_flush_pipeline(cpuid);
#if defined(CONFIG_SMP) && LINUX_VERSION_CODE < KERNEL_VERSION(2,6,19)
	__set_bit(IPIPE_STALL_FLAG, ROOT_STATUS_ADR(rtai_cpuid()));
#endif
	return 1;
}

//#define HINT_DIAG_ECHO
//#define HINT_DIAG_TRAPS

#ifdef HINT_DIAG_ECHO
#define HINT_DIAG_MSG(x) x
#else
#define HINT_DIAG_MSG(x)
#endif

#ifdef UNWRAPPED_CATCH_EVENT
static int rtai_trap_fault (unsigned event, void *evdata)
{
	return 0;
}
#else
static void rtai_trap_fault (adevinfo_t *evinfo) {
	hal_propagate_event(evinfo);
}
#endif

static void rtai_lsrq_dispatcher (unsigned virq)
{
	unsigned long pending, srq;

	spin_lock(&rtai_lsrq_lock);
	while ((pending = rtai_sysreq_pending & ~rtai_sysreq_running)) {
		set_bit(srq = ffnz(pending), &rtai_sysreq_running);
		clear_bit(srq, &rtai_sysreq_pending);
		spin_unlock(&rtai_lsrq_lock);
		if (test_bit(srq, &rtai_sysreq_map)) {
			rtai_sysreq_table[srq].k_handler();
		}
		clear_bit(srq, &rtai_sysreq_running);
		spin_lock(&rtai_lsrq_lock);
	}
	spin_unlock(&rtai_lsrq_lock);
}

static inline long long rtai_usrq_dispatcher (unsigned long srq, unsigned long label)
{
	TRACE_RTAI_SRQ_ENTRY(srq);
	if (srq > 1 && srq < RTAI_NR_SRQS && test_bit(srq, &rtai_sysreq_map) && rtai_sysreq_table[srq].u_handler) {
		return rtai_sysreq_table[srq].u_handler(label);
	} else {
		for (srq = 2; srq < RTAI_NR_SRQS; srq++) {
			if (test_bit(srq, &rtai_sysreq_map) && rtai_sysreq_table[srq].label == label) {
				return (long long)srq;
			}
		}
	}
	TRACE_RTAI_SRQ_EXIT();
	return 0LL;
}

#include <asm/rtai_usi.h>
long long (*rtai_lxrt_dispatcher)(unsigned long, unsigned long, void *);

static int (*sched_intercept_syscall_prologue)(struct pt_regs *);

static int intercept_syscall_prologue(unsigned long event, struct pt_regs *regs){
        if (likely(regs->LINUX_SYSCALL_NR >= RTAI_SYSCALL_NR)) {
                unsigned long srq  = regs->LINUX_SYSCALL_REG1;
		IF_IS_A_USI_SRQ_CALL_IT(srq, regs->LINUX_SYSCALL_REG2, (long long *)regs->LINUX_SYSCALL_REG3, regs->LINUX_SYSCALL_FLAGS, 1);
		*((long long *)regs->LINUX_SYSCALL_REG3) = srq > RTAI_NR_SRQS ?
rtai_lxrt_dispatcher(srq, regs->LINUX_SYSCALL_REG2, regs) : rtai_usrq_dispatcher(srq, regs->LINUX_SYSCALL_REG2);
                if (!in_hrt_mode(srq = rtai_cpuid())) {
			hal_test_and_fast_flush_pipeline(srq);
			return 0;
                }
                return 1;
        } 
	if (likely(sched_intercept_syscall_prologue != NULL)) {
		if (sched_intercept_syscall_prologue(regs)) {
	                return 1;
		}
	}
	return 0;
}

int rtai_syscall_dispatcher (struct pt_regs *regs)
{
	unsigned long srq = regs->rax;
	IF_IS_A_USI_SRQ_CALL_IT(srq, regs->rcx, (long long *)regs->rdx, regs->eflags, 1);
	*((long long *)regs->rdx) = srq > RTAI_NR_SRQS ? rtai_lxrt_dispatcher(srq, regs->rcx, regs) : rtai_usrq_dispatcher(srq, regs->rcx);
	if (!in_hrt_mode(srq = rtai_cpuid())) {
		hal_test_and_fast_flush_pipeline(srq);
		return 1;
	}
	return 0;
}

#if 0
void rtai_uvec_handler (void);
        __asm__ ( \
        "\n" __ALIGN_STR"\n\t" \
        SYMBOL_NAME_STR(rtai_uvec_handler) ":\n\t" \
	"\n .p2align\n" \
	"pushq $0\n" \
	"cld\n" \
	"sub    $0x48,%rsp\n" \
	"mov    %rdi,0x40(%rsp)\n" \
	"mov    %rsi,0x38(%rsp)\n" \
	"mov    %rdx,0x30(%rsp)\n" \
	"mov    %rcx,0x28(%rsp)\n" \
	"mov    %rax,0x20(%rsp)\n" \
	"mov    %r8,0x18(%rsp)\n" \
	"mov    %r9,0x10(%rsp)\n" \
	"mov    %r10,0x8(%rsp)\n" \
	"mov    %r11,(%rsp)\n" \
	"lea    0xffffffffffffffd0(%rsp),%rdi\n" \
	"testl  $0x3,0x88(%rdi)\n" \
	"je 1f\n" \
	"swapgs\n" \
	"1:addl   $0x1,%gs:0x30\n" \
	"mov    %gs:0x38,%rax\n" \
	"cmove  %rax,%rsp\n" \
	"push   %rdi\n" \
	"sti\n" \
	"call "SYMBOL_NAME_STR(rtai_syscall_dispatcher)"\n" \
	"cli\n" \
	"test   %rax,%rax\n" \
	"jnz  ret_from_intr\n" \
	"pop    %rdi\n" \
	"subl   $0x1,%gs:0x30\n" \
	"lea    0x30(%rdi),%rsp\n" \
	"mov    %gs:0x18,%rcx\n" \
	"sub    $0x1fd8,%rcx\n" \
	"testl  $0x3,0x58(%rsp)\n" \
	"je 2f\n" \
	"swapgs\n" \
	"2:mov    (%rsp),%r11\n" \
	"mov    0x8(%rsp),%r10\n" \
	"mov    0x10(%rsp),%r9\n" \
	"mov    0x18(%rsp),%r8\n" \
	"mov    0x20(%rsp),%rax\n" \
	"mov    0x28(%rsp),%rcx\n" \
	"mov    0x30(%rsp),%rdx\n" \
	"mov    0x38(%rsp),%rsi\n" \
	"mov    0x40(%rsp),%rdi\n" \
	"add    $0x50,%rsp\n" \
	"iretq");
#else
void rtai_uvec_handler (void);
        __asm__ ( \
        "\n" __ALIGN_STR"\n\t" \
        SYMBOL_NAME_STR(rtai_uvec_handler) ":\n\t" \
        "\n .p2align\n" \
	"pushq $0\n" \
	"cld\n" \
	"pushq %rdi\n" \
	"pushq %rsi\n" \
	"pushq %rdx\n" \
	"pushq %rcx\n" \
	"pushq %rax\n" \
	"pushq %r8\n" \
	"pushq %r9\n" \
	"pushq %r10\n" \
	"pushq %r11\n" \
	"pushq %rbx\n" \
	"pushq %rbp\n" \
	"pushq %r12\n" \
	"pushq %r13\n" \
	"pushq %r14\n" \
	"pushq %r15\n" \
	"movq %rsp, %rdi\n" \
	"movq %rsp, %rbp\n" \
	"testl $3, 136(%rdi)\n" \
	"je 1f\n" \
	"swapgs\n" \
	"1: movq %gs:56, %rax\n" \
	"cmoveq %rax, %rsp\n" \
	"pushq %rdi\n" \
	"sti\n" \
	"call "SYMBOL_NAME_STR(rtai_syscall_dispatcher)"\n" \
	"cli\n" \
	"popq %rdi\n" \
	"testl $3, 136(%rdi)\n" \
        "je 2f\n" \
	"swapgs\n" \
	"2:\n" \
	"movq %rdi, %rsp\n" \
	"popq %r15\n" \
	"popq %r14\n" \
	"popq %r13\n" \
	"popq %r12\n" \
	"popq %rbp\n" \
	"popq %rbx\n" \
	"popq %r11\n" \
	"popq %r10\n" \
	"popq %r9\n" \
	"popq %r8\n" \
	"popq %rax\n" \
	"popq %rcx\n" \
	"popq %rdx\n" \
	"popq %rsi\n" \
	"popq %rdi\n" \
	"addq $8, %rsp\n" \
        "iretq");
#endif

struct gate_struct rtai_set_gate_vector (unsigned vector, int type, int dpl, void *handler)
{
	struct gate_struct e = idt_table[vector];
	_set_gate(&idt_table[vector], type, (unsigned long)handler, dpl, 0);
	return e;
}

void rtai_reset_gate_vector (unsigned vector, struct gate_struct e)
{
	idt_table[vector] = e;
}

static void rtai_install_archdep (void)
{
#ifndef RTAI_SYSCALLS_USE_HIRQ_DISPATCHER
	unsigned long flags;

	flags = rtai_critical_enter(NULL);
    /* Backup and replace the sysreq vector. */
	rtai_sysvec = rtai_set_gate_vector(RTAI_SYS_VECTOR, GATE_INTERRUPT, 3, rtai_uvec_handler);
	rtai_critical_exit(flags);
#else
//	idt_table[RTAI_SYS_VECTOR].type = 0xE;
	idt_table[RTAI_SYS_VECTOR].dpl  = 0x3;
	rt_request_irq(RTAI_SYS_IRQ, (void *)rtai_syscall_dispatcher, 0, 1);
#endif

        hal_catch_event(hal_root_domain, HAL_SYSCALL_PROLOGUE, (void *)intercept_syscall_prologue);

	if (rtai_cpufreq_arg == 0) {
		struct hal_sysinfo_struct sysinfo;
		hal_get_sysinfo(&sysinfo);
		rtai_cpufreq_arg = (unsigned long)sysinfo.cpufreq;
	}
	rtai_tunables.cpu_freq = rtai_cpufreq_arg;

#ifdef CONFIG_X86_LOCAL_APIC
	if (rtai_apicfreq_arg == 0) {
		rtai_apicfreq_arg = HZ*apic_read(APIC_TMICT);
	}
	rtai_tunables.apic_freq = rtai_apicfreq_arg;
#endif /* CONFIG_X86_LOCAL_APIC */
}

static void rtai_uninstall_archdep(void)
{
#ifndef RTAI_SYSCALLS_USE_HIRQ_DISPATCHER
	unsigned long flags;

	flags = rtai_critical_enter(NULL);
	idt_table[RTAI_SYS_VECTOR] = rtai_sysvec;
	rtai_critical_exit(flags);
#else
	rt_release_irq(RTAI_SYS_IRQ);
//	idt_table[RTAI_SYS_VECTOR].type = 0xE;
	idt_table[RTAI_SYS_VECTOR].dpl  = 0x0;
#endif

	hal_catch_event(hal_root_domain, HAL_SYSCALL_PROLOGUE, NULL);
}

int rtai_calibrate_8254 (void)
{
	unsigned long flags;
	RTIME t, dt;
	int i;

	flags = rtai_critical_enter(NULL);
	outb(0x34,0x43);
	t = rtai_rdtsc();
	for (i = 0; i < 10000; i++) { 
		outb(LATCH & 0xff,0x40);
		outb(LATCH >> 8,0x40);
	}
	dt = rtai_rdtsc() - t;
	rtai_critical_exit(flags);

	return rtai_imuldiv(dt, 100000, RTAI_CPU_FREQ);
}

void (*rt_set_ihook (void (*hookfn)(int)))(int)
{
#ifdef CONFIG_RTAI_SCHED_ISR_LOCK
	return (void (*)(int))xchg(&rtai_isr_hook, hookfn); /* This is atomic */
#else  /* !CONFIG_RTAI_SCHED_ISR_LOCK */
	return NULL;
#endif /* CONFIG_RTAI_SCHED_ISR_LOCK */
}


#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,11)
//static int errno;

//static _syscall3(asmlinkage int, sched_setscheduler, pid_t, pid, int, policy, struct sched_param *, param)

extern void *sys_call_table[];

void rtai_set_linux_task_priority (struct task_struct *task, int policy, int prio)
{
        int rc;
        struct sched_param __user param;
        mm_segment_t old_fs;

        param.sched_priority = prio;
        old_fs = get_fs();
        set_fs(KERNEL_DS);

        rc = ((int (*)(int, int, struct sched_param *))sys_call_table[__NR_sched_setscheduler])(task->pid, policy, &param);

//	rc = sched_setscheduler(task->pid, policy, &param);
        set_fs(old_fs);

//#else
//        struct sched_param param = { prio };
//        rc = sched_setscheduler(task, policy, &param);
//#endif

        if (rc) {
                printk("RTAI[hal]: sched_setscheduler(policy=%d,prio=%d) failed, code %d (%s -- pid=%d)\n", policy, prio, rc, task->comm, task->pid);
        }

	return;
}

#else

void rtai_set_linux_task_priority (struct task_struct *task, int policy, int prio)
{
        hal_set_linux_task_priority(task, policy, prio);
        if (task->rt_priority != prio || task->policy != policy) {
                printk("RTAI[hal]: sched_setscheduler(policy = %d, prio = %d) failed, (%s -- pid = %d)\n", policy, prio, task->comm, task->pid);
        }

}

#endif

#ifdef CONFIG_PROC_FS

struct proc_dir_entry *rtai_proc_root = NULL;

static int rtai_read_proc (char *page, char **start, off_t off, int count, int *eof, void *data)
{
	PROC_PRINT_VARS;
	int i, none;

	PROC_PRINT("\n** RTAI/x86:\n\n");
#ifdef CONFIG_X86_LOCAL_APIC
	PROC_PRINT("    APIC Frequency: %lu\n",rtai_tunables.apic_freq);
	PROC_PRINT("    APIC Latency: %d ns\n",RTAI_LATENCY_APIC);
	PROC_PRINT("    APIC Setup: %d ns\n",RTAI_SETUP_TIME_APIC);
#endif /* CONFIG_X86_LOCAL_APIC */
    
	none = 1;
	PROC_PRINT("\n** Real-time IRQs used by RTAI: ");
    	for (i = 0; i < RTAI_NR_IRQS; i++) {
		if (rtai_realtime_irq[i].handler) {
			if (none) {
				PROC_PRINT("\n");
				none = 0;
			}
			PROC_PRINT("\n    #%d at %p", i, rtai_realtime_irq[i].handler);
		}
        }
	if (none) {
		PROC_PRINT("none");
	}
	PROC_PRINT("\n\n");

	PROC_PRINT("** RTAI extension traps: \n\n");
	PROC_PRINT("    SYSREQ=0x%x\n", RTAI_SYS_VECTOR);

	none = 1;
	PROC_PRINT("** RTAI SYSREQs in use: ");
    	for (i = 0; i < RTAI_NR_SRQS; i++) {
		if (rtai_sysreq_table[i].k_handler || rtai_sysreq_table[i].u_handler) {
			PROC_PRINT("#%d ", i);
			none = 0;
		}
        }

	if (none) {
		PROC_PRINT("none");
	}
    	PROC_PRINT("\n\n");
	PROC_PRINT_DONE;
}

static int rtai_proc_register (void)
{
	struct proc_dir_entry *ent;

	rtai_proc_root = create_proc_entry("rtai",S_IFDIR, 0);
	if (!rtai_proc_root) {
		printk(KERN_ERR "Unable to initialize /proc/rtai.\n");
		return -1;
        }
	rtai_proc_root->owner = THIS_MODULE;
	ent = create_proc_entry("hal",S_IFREG|S_IRUGO|S_IWUSR,rtai_proc_root);
	if (!ent) {
		printk(KERN_ERR "Unable to initialize /proc/rtai/hal.\n");
		return -1;
        }
	ent->read_proc = rtai_read_proc;

	return 0;
}

static void rtai_proc_unregister (void)
{
	remove_proc_entry("hal",rtai_proc_root);
	remove_proc_entry("rtai",0);
}

#endif /* CONFIG_PROC_FS */

FIRST_LINE_OF_RTAI_DOMAIN_ENTRY
{
	{
		rt_printk(KERN_INFO "RTAI[hal]: <%s> mounted over %s %s.\n", PACKAGE_VERSION, HAL_TYPE, HAL_VERSION_STRING);
		rt_printk(KERN_INFO "RTAI[hal]: compiled with %s.\n", CONFIG_RTAI_COMPILER);
	}
	for (;;) hal_suspend_domain();
}
LAST_LINE_OF_RTAI_DOMAIN_ENTRY

long rtai_catch_event (struct hal_domain_struct *from, unsigned long event, int (*handler)(unsigned long, void *))
{
        if (event == HAL_SYSCALL_PROLOGUE) {
                sched_intercept_syscall_prologue = (void *)handler;
                return 0;
        }
	return (long)hal_catch_event(from, event, (void *)handler);
}

static void *saved_hal_irq_handler;
extern void *hal_irq_handler;

#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,11)
static inline void *hal_set_irq_handler(void *hirq_dispatcher)
{
	if (saved_hal_irq_handler != hirq_dispatcher) {
		saved_hal_irq_handler = hal_irq_handler;
		hal_irq_handler = hirq_dispatcher;
		return saved_hal_irq_handler;
	}
	hal_irq_handler = hirq_dispatcher;
	return rtai_hirq_dispatcher;
}

#undef ack_bad_irq
void ack_bad_irq(unsigned int irq)
{
        printk("unexpected IRQ trap at vector %02x\n", irq);
#ifdef CONFIG_X86_LOCAL_APIC
        if (cpu_has_apic) {
                __ack_APIC_irq();
        }
#endif
}
#endif

static void rt_printk_srq_handler(void);
#define RT_PRINTK_SRQ  1

int __rtai_hal_init (void)
{
	int trapnr, halinv;
	struct hal_attr_struct attr;

	for (halinv = trapnr = 0; trapnr < HAL_NR_EVENTS; trapnr++) {
		if (hal_root_domain->hal_event_handler_fun(trapnr)) {
			halinv = 1;
			printk("EVENT %d INVALID\n", trapnr);
		}
	}
	if (halinv) {
		printk(KERN_ERR "RTAI[hal]: HAL IMMEDIATE EVENT DISPATCHING BROKEN\n");
		return -1;
	}

#ifdef CONFIG_X86_LOCAL_APIC
	if (!test_bit(X86_FEATURE_APIC, boot_cpu_data.x86_capability)) {
		printk("RTAI[hal]: ERROR, LOCAL APIC CONFIGURED BUT NOT AVAILABLE/ENABLED\n");
		return -1;
	}
#endif

	if (!(rtai_sysreq_virq = hal_alloc_irq())) {
		printk(KERN_ERR "RTAI[hal]: no virtual interrupt available.\n");
		return 1;
	}

        for (trapnr = 0; trapnr < RTAI_NR_IRQS; trapnr++) {
                rtai_realtime_irq[trapnr].irq_ack = hal_root_domain->irqs[trapnr].acknowledge;
        }
#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
        for (trapnr = 0; trapnr < RTAI_NR_CPUS; trapnr++) {
                ipipe_root_status[trapnr] = &hal_root_domain->cpudata[trapnr].status;
        }
#endif

	hal_virtualize_irq(hal_root_domain, rtai_sysreq_virq, &rtai_lsrq_dispatcher, NULL, IPIPE_HANDLE_MASK);

	saved_hal_irq_handler = hal_set_irq_handler(rtai_hirq_dispatcher);

	rtai_install_archdep();

#ifdef CONFIG_PROC_FS
	rtai_proc_register();
#endif

	rtai_sysreq_table[RT_PRINTK_SRQ].k_handler = rt_printk_srq_handler;
	set_bit(RT_PRINTK_SRQ, &rtai_sysreq_map);

	hal_init_attr(&attr);
	attr.name     = "RTAI";
	attr.domid    = RTAI_DOMAIN_ID;
	attr.entry    = (void *)rtai_domain_entry;
	attr.priority = get_domain_pointer(1)->priority + 100;
	hal_register_domain(&rtai_domain, &attr);
	for (trapnr = 0; trapnr < HAL_NR_FAULTS; trapnr++) {
		hal_catch_event(hal_root_domain, trapnr, (void *)rtai_trap_fault);
	}

	rtai_init_taskpri_irqs();

#ifdef CONFIG_SMP
	if (IsolCpusMask) {
		for (trapnr = 0; trapnr < IPIPE_NR_XIRQS; trapnr++) {
			rtai_orig_irq_affinity[trapnr] = rt_assign_irq_to_cpu(trapnr, ~IsolCpusMask);
		}
	}
#else
	IsolCpusMask = 0;
#endif

	printk(KERN_INFO "RTAI[hal]: mounted (%s, IMMEDIATE (INTERNAL IRQs %s), ISOL_CPUS_MASK: %lx).\n", HAL_TYPE, CONFIG_RTAI_DONT_DISPATCH_CORE_IRQS ? "VECTORED" : "DISPATCHED", IsolCpusMask);

	printk("PIPELINE layers:\n");
	for (trapnr = 1; ; trapnr++) {
		struct hal_domain_struct *next_domain;
		next_domain = get_domain_pointer(trapnr);
		if ((unsigned long)next_domain < 10) break;
		printk("%p %x %s %d\n", next_domain, next_domain->domid, next_domain->name, next_domain->priority);
	}

	return 0;
}

void __rtai_hal_exit (void)
{
	int trapnr;
#ifdef CONFIG_PROC_FS
	rtai_proc_unregister();
#endif
	hal_set_irq_handler(saved_hal_irq_handler);

	hal_unregister_domain(&rtai_domain);
	for (trapnr = 0; trapnr < HAL_NR_FAULTS; trapnr++) {
		hal_catch_event(hal_root_domain, trapnr, NULL);
	}

	clear_bit(RT_PRINTK_SRQ, &rtai_sysreq_map);
        hal_virtualize_irq(hal_root_domain, rtai_sysreq_virq, NULL, NULL, 0);
        hal_free_irq(rtai_sysreq_virq);
        rtai_uninstall_archdep();
	
	if (IsolCpusMask) {
		for (trapnr = 0; trapnr < IPIPE_NR_XIRQS; trapnr++) {
			rt_reset_irq_to_sym_mode(trapnr);
		}
	}

	printk(KERN_INFO "RTAI[hal]: unmounted.\n");
}

module_init(__rtai_hal_init);
module_exit(__rtai_hal_exit);

#if 1
/*
 *  rt_printk.c, hacked from linux/kernel/printk.c.
 *
 * Modified for RT support, David Schleef.
 *
 * Adapted to RTAI, and restyled his own way by Paolo Mantegazza.
 *
 */

#define PRINTK_BUF_SIZE  (10000) // Test programs may generate much output. PC
#define TEMP_BUF_SIZE	 (500)

static char rt_printk_buf[PRINTK_BUF_SIZE];

static int buf_front, buf_back;
static char buf[TEMP_BUF_SIZE];

int rt_printk (const char *fmt, ...)
{
	unsigned long flags;
        static spinlock_t display_lock = SPIN_LOCK_UNLOCKED;
	va_list args;
	int len, i;

        flags = rt_spin_lock_irqsave(&display_lock);
	va_start(args, fmt);
	len = vsprintf(buf, fmt, args);
	va_end(args);
	if ((buf_front + len) >= PRINTK_BUF_SIZE) {
		i = PRINTK_BUF_SIZE - buf_front;
		memcpy(rt_printk_buf + buf_front, buf, i);
		memcpy(rt_printk_buf, buf + i, len - i);
		buf_front = len - i;
	} else {
		memcpy(rt_printk_buf + buf_front, buf, len);
		buf_front += len;
	}
        rt_spin_unlock_irqrestore(flags, &display_lock);
	rt_pend_linux_srq(RT_PRINTK_SRQ);

	return len;
}

static void rt_printk_srq_handler (void)
{
	int tmp;

	while(1) {
		tmp = buf_front;
		if (buf_back > tmp) {
			printk("%.*s", PRINTK_BUF_SIZE - buf_back, rt_printk_buf + buf_back);
			buf_back = 0;
		}
		if (buf_back == tmp) {
			break;
		}
		printk("%.*s", tmp - buf_back, rt_printk_buf + buf_back);
		buf_back = tmp;
	}
}
#else
asmlinkage int rt_printk(const char *fmt, ...)
{
        va_list args;
        int r;

        va_start(args, fmt);
        r = vprintk(fmt, args);
        va_end(args);

        return r;
}
static void rt_printk_srq_handler (void) { }
#endif

EXPORT_SYMBOL(rtai_realtime_irq);
EXPORT_SYMBOL(rt_request_irq);
EXPORT_SYMBOL(rt_release_irq);
EXPORT_SYMBOL(rt_set_irq_cookie);
EXPORT_SYMBOL(rt_set_irq_retmode);
EXPORT_SYMBOL(rt_startup_irq);
EXPORT_SYMBOL(rt_shutdown_irq);
EXPORT_SYMBOL(rt_enable_irq);
EXPORT_SYMBOL(rt_disable_irq);
EXPORT_SYMBOL(rt_mask_and_ack_irq);
EXPORT_SYMBOL(rt_unmask_irq);
EXPORT_SYMBOL(rt_ack_irq);
EXPORT_SYMBOL(rt_end_irq);
EXPORT_SYMBOL(rt_eoi_irq);
EXPORT_SYMBOL(rt_request_linux_irq);
EXPORT_SYMBOL(rt_free_linux_irq);
EXPORT_SYMBOL(rt_pend_linux_irq);
EXPORT_SYMBOL(rt_request_srq);
EXPORT_SYMBOL(rt_free_srq);
EXPORT_SYMBOL(rt_pend_linux_srq);
EXPORT_SYMBOL(usr_rt_pend_linux_irq);
EXPORT_SYMBOL(rt_assign_irq_to_cpu);
EXPORT_SYMBOL(rt_reset_irq_to_sym_mode);
EXPORT_SYMBOL(rt_request_apic_timers);
EXPORT_SYMBOL(rt_free_apic_timers);
EXPORT_SYMBOL(rt_request_timer);
EXPORT_SYMBOL(rt_free_timer);
EXPORT_SYMBOL(rt_set_trap_handler);
EXPORT_SYMBOL(rd_8254_ts);
EXPORT_SYMBOL(rt_setup_8254_tsc);
EXPORT_SYMBOL(rt_set_ihook);
EXPORT_SYMBOL(rt_set_irq_ack);
//EXPORT_SYMBOL(ack_8259A_irq);

EXPORT_SYMBOL(rtai_calibrate_8254);
EXPORT_SYMBOL(rtai_broadcast_to_local_timers);
EXPORT_SYMBOL(rtai_critical_enter);
EXPORT_SYMBOL(rtai_critical_exit);
EXPORT_SYMBOL(rtai_set_linux_task_priority);

EXPORT_SYMBOL(rtai_linux_context);
EXPORT_SYMBOL(rtai_domain);
EXPORT_SYMBOL(rtai_proc_root);
EXPORT_SYMBOL(rtai_tunables);
EXPORT_SYMBOL(rtai_cpu_lock);
EXPORT_SYMBOL(rtai_cpu_realtime);
EXPORT_SYMBOL(rt_times);
EXPORT_SYMBOL(rt_smp_times);

EXPORT_SYMBOL(rt_printk);

EXPORT_SYMBOL(rtai_set_gate_vector);
EXPORT_SYMBOL(rtai_reset_gate_vector);
EXPORT_SYMBOL(rtai_catch_event);

EXPORT_SYMBOL(rtai_lxrt_dispatcher);
EXPORT_SYMBOL(rt_scheduling);
#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
EXPORT_SYMBOL(ipipe_root_status);
#endif


#ifdef CONFIG_SMP
EXPORT_SYMBOL(rt_set_sched_ipi_gate);
EXPORT_SYMBOL(rt_reset_sched_ipi_gate);
#endif

/*@}*/

#ifdef CONFIG_GENERIC_CLOCKEVENTS

#include <linux/clockchips.h>
#include <linux/ipipe_tickdev.h>

void (*rt_linux_hrt_set_mode)(enum clock_event_mode, struct ipipe_tick_device *);
int (*rt_linux_hrt_next_shot)(unsigned long, struct ipipe_tick_device *);

#ifdef CONFIG_SMP
#define TEST_LINUX_TICK  RTAI_APIC_ICOUNT
#else
#define TEST_LINUX_TICK  (used_apic ? RTAI_APIC_ICOUNT : LATCH)
#endif 

/* 
 * _rt_linux_hrt_set_mode and _rt_linux_hrt_next_shot below should serve 
 * RTAI examples only and assume that RTAI is in periodic mode always
 */

static void _rt_linux_hrt_set_mode(enum clock_event_mode mode, struct ipipe_tick_device *hrt_dev)
{
	if (mode == CLOCK_EVT_MODE_ONESHOT || mode == CLOCK_EVT_MODE_SHUTDOWN) {
		rt_times.linux_tick = 0;
	} else if (mode == CLOCK_EVT_MODE_PERIODIC) {
		rt_times.linux_tick = llimd((1000000000 + HZ/2)/HZ, TIMER_FREQ, 1000000000);
	}
}

static int _rt_linux_hrt_next_shot(unsigned long delay, struct ipipe_tick_device *hrt_dev)
{
	rt_times.linux_time = rt_times.tick_time + llimd(delay, TIMER_FREQ, 1000000000);
	return 0;
}

static int rtai_request_tickdev(void)
{
	int mode, cpuid;
	for (cpuid = 0; cpuid < num_online_cpus(); cpuid++) {
		if ((void *)rt_linux_hrt_set_mode != (void *)rt_linux_hrt_next_shot) {
			mode = ipipe_request_tickdev(HRT_LINUX_TIMER_NAME, rt_linux_hrt_set_mode, rt_linux_hrt_next_shot, cpuid);
		} else {
			mode = ipipe_request_tickdev(HRT_LINUX_TIMER_NAME, _rt_linux_hrt_set_mode, _rt_linux_hrt_next_shot, cpuid);
		}
		if (mode == CLOCK_EVT_MODE_PERIODIC) {
//			rt_times.linux_tick = TEST_LINUX_TICK;
//			rt_times.linux_tick = nano2count_cpuid((1000000000 + HZ/2)/HZ, cpuid);
		} else if (mode == CLOCK_EVT_MODE_UNUSED || mode == CLOCK_EVT_MODE_ONESHOT) {
			rt_times.linux_tick = 0;
		} else {
			return mode;
		}
	}
	return 0;
}

static void rtai_release_tickdev(void)
{
	int cpuid;
	for (cpuid = 0; cpuid < num_online_cpus(); cpuid++) {
		ipipe_release_tickdev(cpuid);
	}
}

#else /* !CONFIG_GENERIC_CLOCKEVENTS */

void (*rt_linux_hrt_set_mode)(int clock_event_mode, void *);
int (*rt_linux_hrt_next_shot)(unsigned long, void *);

static int rtai_request_tickdev(void)   { return 0; }

static void rtai_release_tickdev(void)  {  return;  }

#endif /* CONFIG_GENERIC_CLOCKEVENTS */

EXPORT_SYMBOL(rt_linux_hrt_set_mode);
EXPORT_SYMBOL(rt_linux_hrt_next_shot);

EXPORT_SYMBOL(IsolCpusMask);
